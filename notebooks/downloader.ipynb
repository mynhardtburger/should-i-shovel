{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from typing import Any, Iterable, Mapping, Optional, Sequence, TypeAlias, Union\n",
    "\n",
    "import dateparser\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import pandas as pd\n",
    "import psycopg\n",
    "import pygrib\n",
    "import requests\n",
    "import xarray as xr\n",
    "from psycopg import sql\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3 import Retry\n",
    "\n",
    "Query: TypeAlias = Union[bytes, \"sql.SQL\", \"sql.Composed\"]\n",
    "Params: TypeAlias = Union[Sequence[Any], Mapping[str, Any]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"CMC_hrdps_domain_Variable_LevelType_level_ps2.5km_YYYYMMDDHH_Phhh-mm.grib2\"  \n",
    "\"CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022121900_P000-00.grib2 \"  \n",
    "\"https://dd.weather.gc.ca/model_hrdps/continental/grib2/00/000/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022121900_P000-00.grib2\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_urls() -> list[list[str]]:\n",
    "    base_url = \"https://dd.weather.gc.ca/model_hrdps/continental/grib2/\"\n",
    "    model_runs = [f\"{(i):0>2}\" for i in range(0, 24, 6)]\n",
    "    forecast_hours = [f\"{(i):0>3}\" for i in range(49)]\n",
    "    prefix = \"CMC\"\n",
    "    model = \"hrdps\"\n",
    "    domain = \"continental\"\n",
    "    variable = \"SNOD\"\n",
    "    level_type = \"SFC\"\n",
    "    level = \"0\"\n",
    "    resolution = \"ps2.5km\"\n",
    "    date = datetime.now(timezone.utc).strftime(\"%Y%m%d\")\n",
    "    minutes = \"00\"\n",
    "    extension = \"grib2\"\n",
    "\n",
    "    model_run_urls = []\n",
    "    for model_run in model_runs:\n",
    "        prediction_urls = []\n",
    "        for forecast_hour in forecast_hours:\n",
    "            filename = f\"{prefix}_{model}_{domain}_{variable}_{level_type}_{level}_{resolution}_{date}{model_run}_P{forecast_hour}-{minutes}.{extension}\"\n",
    "            download_url = f\"{base_url}{model_run}/{forecast_hour}/{filename}\"\n",
    "            prediction_urls.append(download_url)\n",
    "        model_run_urls.append(prediction_urls)\n",
    "    \n",
    "    return model_run_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_run_urls = create_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_latest_run(model_run_urls: list[list[str]]) -> int:\n",
    "    \"\"\"Find the latest model run by issuing a HEAD http request to the first url of each model run and comparing the \"Last-Modified\" field.\"\"\"\n",
    "    latest_time = datetime.fromisoformat(\"0001-01-01 00:00:00.000+00:00\")\n",
    "    latest_idx = 0\n",
    "    for idx, prediction_urls in enumerate(model_run_urls):\n",
    "        res = requests.head(prediction_urls[0])\n",
    "        if not res.ok:\n",
    "            continue\n",
    "\n",
    "        modified_date = dateparser.parse(res.headers[\"Last-Modified\"])\n",
    "        if not modified_date:\n",
    "            continue\n",
    "\n",
    "        if modified_date > latest_time:\n",
    "            latest_time = modified_date\n",
    "            latest_idx = idx\n",
    "\n",
    "    return latest_idx\n",
    "\n",
    "\n",
    "latest_id = find_latest_run(model_run_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "def download_predictions(model_run_urls: list[list[str]], model_idx:int = 0, savepath:str = \"./\") -> list[str]:\n",
    "    filepaths = []\n",
    "\n",
    "    retry_strategy = Retry(\n",
    "        total=3,\n",
    "        backoff_factor=0.3,\n",
    "        status_forcelist=[404, 429, 500, 502, 503, 504],\n",
    "        method_whitelist=[\"HEAD\", \"GET\", \"OPTIONS\"]\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "    http = requests.Session()\n",
    "    http.mount(\"https://\", adapter)\n",
    "    http.mount(\"http://\", adapter)\n",
    "\n",
    "    for prediction_url in model_run_urls[model_idx]:\n",
    "        filename = prediction_url.split(\"/\")[-1]\n",
    "        print(f\"Downloading {prediction_url}\")\n",
    "        res = http.get(prediction_url)\n",
    "        filepath = os.path.join(savepath, filename)\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            f.write(res.content)\n",
    "        filepaths.append(filepath)\n",
    "    return filepaths\n",
    "\n",
    "paths = download_predictions(model_run_urls, latest_id, \"../data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_predictions(model_run_urls: list[list[str]], model_idx:int = 0, savepath:str = \"./\") -> list[str]:\n",
    "    filepaths = []\n",
    "    for prediction_url in model_run_urls[model_idx]:\n",
    "        filename = prediction_url.split(\"/\")[-1]\n",
    "        print(f\"Processing {prediction_url}\")\n",
    "        res = requests.get(prediction_url)\n",
    "        filepath = os.path.join(savepath, filename)\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            f.write(res.content)\n",
    "        filepaths.append(filepath)\n",
    "    return filepaths\n",
    "\n",
    "paths = download_predictions(model_run_urls, latest_id, \"../data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current time in UTC\n",
    "# for most recent to oldest model run:\n",
    "#   Is date and model run newer than existing data in DB?\n",
    "#       NO > raise error & stop\n",
    "#   retrieve current date's forecast hour folder listing:\n",
    "#       Contains all forecast hours?\n",
    "#            YES > download\n",
    "#            NO or 404 > attempt earlier model run\n",
    "#   Nothing downloaded > go back one day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code: 404 https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/18/048/\n",
      "Success: https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/ 20221221 12\n"
     ]
    }
   ],
   "source": [
    "def query_latest(\n",
    "    forecast_hour: int,\n",
    "    url_path: str = \"WXO-DD/model_hrdps/continental/grib2\",\n",
    "    domain: str = \"https://hpfx.collab.science.gc.ca\",\n",
    ") -> tuple[str, str, str] | None:\n",
    "    \"\"\"Find the latest model run.\n",
    "    Starting at the current date working backwards 5 days, for each day for forecasts 18,12,06,00 a HEAD request is sent to the forecast_hour.\n",
    "    First 200 OK response is returned\"\"\"\n",
    "    current_date = datetime.now(timezone.utc)\n",
    "\n",
    "    # url parameters\n",
    "    forecasts = [f\"{i:02}\" for i in range(0, 24, 6)][::-1]\n",
    "    dates = [(current_date - timedelta(days=i)).strftime(\"%Y%m%d\") for i in range(5)]\n",
    "\n",
    "    for date in dates:\n",
    "        for forecast in forecasts:\n",
    "            baseurl = f\"{domain.strip('/')}/{date}/{url_path.strip('/')}/{forecast}/\"\n",
    "            request_url = f\"{baseurl}{forecast_hour:03}/\"\n",
    "            res = requests.head(request_url)\n",
    "            if res.status_code == 200:\n",
    "                print(\"Success:\", baseurl, date, forecast)\n",
    "                return baseurl, date, forecast\n",
    "            else:\n",
    "                print(\"Status code:\", res.status_code, request_url)\n",
    "\n",
    "\n",
    "baseurl, date, model_run = query_latest(48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_urls(\n",
    "    base_url:str = \"https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/06/\",\n",
    "    forecast_hours:list[str] = [f\"{(i):03}\" for i in range(49)],\n",
    "    prefix:str = \"CMC\",\n",
    "    model:str = \"hrdps\",\n",
    "    domain:str = \"continental\",\n",
    "    variable:str = \"SNOD\",\n",
    "    level_type:str = \"SFC\",\n",
    "    level:str = \"0\",\n",
    "    resolution:str = \"ps2.5km\",\n",
    "    date:str = datetime.now(timezone.utc).strftime(\"%Y%m%d\"),\n",
    "    model_run:str = \"00\",\n",
    "    minutes:str = \"00\",\n",
    "    extension:str = \"grib2\",\n",
    "    ) -> list[str]:\n",
    "    \"\"\"Create list of URLs given inputs.\n",
    "    URL structure: {base_url}/{forecast_hour}/{filename}\n",
    "    Filename structure: {prefix}_{model}_{domain}_{variable}_{level_type}_{level}_{resolution}_{date}{model_run}_P{forecast_hour}-{minutes}.{extension}\"\"\"\n",
    "    urls = []\n",
    "    for forecast_hour in forecast_hours:\n",
    "        filename = f\"{prefix}_{model}_{domain}_{variable}_{level_type}_{level}_{resolution}_{date}{model_run:02}_P{forecast_hour:03}-{minutes:02}.{extension}\"\n",
    "        download_url = f\"{base_url.strip('/')}/{forecast_hour.strip('/')}/{filename}\"\n",
    "        urls.append(download_url)\n",
    "    \n",
    "    return urls\n",
    "\n",
    "download_urls = create_urls(base_url=baseurl, date=date, model_run=model_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/000/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P000-00.grib2 | File size: 2912404\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/001/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P001-00.grib2 | File size: 2917492\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/002/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P002-00.grib2 | File size: 2916855\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/003/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P003-00.grib2 | File size: 2915974\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/004/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P004-00.grib2 | File size: 2915240\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/005/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P005-00.grib2 | File size: 2913825\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/006/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P006-00.grib2 | File size: 2912825\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/007/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P007-00.grib2 | File size: 2910136\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/008/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P008-00.grib2 | File size: 2908273\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/009/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P009-00.grib2 | File size: 2906532\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/010/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P010-00.grib2 | File size: 2905761\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/011/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P011-00.grib2 | File size: 2904690\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/012/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P012-00.grib2 | File size: 2904456\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/013/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P013-00.grib2 | File size: 2903964\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/014/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P014-00.grib2 | File size: 2903510\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/015/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P015-00.grib2 | File size: 2903212\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/016/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P016-00.grib2 | File size: 2902273\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/017/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P017-00.grib2 | File size: 2901363\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/018/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P018-00.grib2 | File size: 2900713\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/019/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P019-00.grib2 | File size: 2900288\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/020/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P020-00.grib2 | File size: 2900007\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/021/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P021-00.grib2 | File size: 2899687\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/022/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P022-00.grib2 | File size: 2899441\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/023/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P023-00.grib2 | File size: 2899626\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/024/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P024-00.grib2 | File size: 2899368\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/025/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P025-00.grib2 | File size: 2898789\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/026/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P026-00.grib2 | File size: 2899789\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/027/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P027-00.grib2 | File size: 2900748\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/028/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P028-00.grib2 | File size: 2901258\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/029/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P029-00.grib2 | File size: 2902032\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/030/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P030-00.grib2 | File size: 2902971\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/031/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P031-00.grib2 | File size: 2903865\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/032/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P032-00.grib2 | File size: 2904959\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/033/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P033-00.grib2 | File size: 2907306\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/034/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P034-00.grib2 | File size: 2908871\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/035/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P035-00.grib2 | File size: 2910318\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/036/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P036-00.grib2 | File size: 2911622\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/037/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P037-00.grib2 | File size: 2913366\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/038/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P038-00.grib2 | File size: 2915459\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/039/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P039-00.grib2 | File size: 2917296\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/040/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P040-00.grib2 | File size: 2918281\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/041/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P041-00.grib2 | File size: 2919805\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/042/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P042-00.grib2 | File size: 2921697\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/043/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P043-00.grib2 | File size: 2923394\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/044/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P044-00.grib2 | File size: 2925576\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/045/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P045-00.grib2 | File size: 2929398\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/046/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P046-00.grib2 | File size: 2934092\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/047/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P047-00.grib2 | File size: 2937420\n",
      "Downloading https://hpfx.collab.science.gc.ca/20221221/WXO-DD/model_hrdps/continental/grib2/12/048/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P048-00.grib2 | File size: 2939688\n"
     ]
    }
   ],
   "source": [
    "def download_predictions(download_urls: list[str], savepath: str = \"./\") -> list[str]:\n",
    "    \"\"\"Download list of urls to savepath\"\"\"\n",
    "    filepaths = []\n",
    "\n",
    "    retry_strategy = Retry(\n",
    "        total=3,\n",
    "        backoff_factor=0.3,\n",
    "        status_forcelist=[404, 429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"],\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "    http = requests.Session()\n",
    "    http.mount(\"https://\", adapter)\n",
    "    http.mount(\"http://\", adapter)\n",
    "\n",
    "    for url in download_urls:\n",
    "        filename = url.split(\"/\")[-1]\n",
    "        print(f\"Downloading {url}\", end=\" | \")\n",
    "        res = http.get(url)\n",
    "        filepath = os.path.join(savepath, filename)\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            f.write(res.content)\n",
    "        print(\"File size:\", res.headers[\"Content-Length\"])\n",
    "        filepaths.append(filepath)\n",
    "    return filepaths\n",
    "\n",
    "\n",
    "paths = download_predictions(download_urls, \"../data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_grib_predictions(filepaths:list[str]) -> list[npt.NDArray]:\n",
    "    predictions:list[npt.NDArray] = []\n",
    "    for path in filepaths:\n",
    "        print(f\"Reading {path}\")\n",
    "        gribs = pygrib.open(path)\n",
    "        data = np.ma.filled(gribs[1].values, 0)\n",
    "        predictions.append(data.reshape(-1))\n",
    "    return predictions\n",
    "\n",
    "data = read_grib_predictions(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_grib_to_df(path: str) -> dict[str, pd.DataFrame | datetime | timedelta]:\n",
    "    \"\"\"Reads GRIB2 file and returns a dictionary with the data in a dataframe and details about the GRIB file.\n",
    "    Multi-message GRIB2 files are not supported.\"\"\"\n",
    "    grib = pygrib.open(path)[1]\n",
    "    data = np.around(\n",
    "        np.stack(\n",
    "            [\n",
    "                *grib.latlons(),\n",
    "                np.ma.filled(grib.values, 0),\n",
    "            ],\n",
    "            axis=-1,\n",
    "        ).reshape((-1, 3)),\n",
    "        decimals=6,\n",
    "    )\n",
    "    # lat, lon = grib.latlons()\n",
    "    forecast_reference_time = datetime.strptime(\n",
    "        f\"{grib['dataDate']}{grib['dataTime']:04} +0000\", \"%Y%m%d%H%M %z\"\n",
    "    )\n",
    "    forecast_validity_time = datetime.strptime(\n",
    "        f\"{grib['validityDate']}{grib['validityTime']:04} +0000\", \"%Y%m%d%H%M %z\"\n",
    "    )\n",
    "    forecast_step = forecast_validity_time - forecast_reference_time\n",
    "    return {\n",
    "        \"data\": pd.DataFrame(data, columns=[\"latitude\", \"longitude\", \"value\"]),\n",
    "        \"forecast_reference_time\": forecast_reference_time,\n",
    "        \"forecast_validity_time\": forecast_validity_time,\n",
    "        \"forecast_step\": forecast_step,\n",
    "        \"short_name\": grib[\"shortName\"],\n",
    "        \"long_name\": grib[\"name\"],\n",
    "        \"units\": grib[\"units\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utf-8\n",
      "[(datetime.datetime(2022, 12, 22, 1, 12, 48, 538132, tzinfo=datetime.timezone.utc),)]\n"
     ]
    }
   ],
   "source": [
    "# Define connection details\n",
    "pg_connection_dict = {\n",
    "    'dbname': \"mydb\",\n",
    "    'user': \"myn\",\n",
    "    'password': r\"2)2K9zJCKZv7pLUd\",\n",
    "    'port': \"5432\",\n",
    "    'host': \"terraform-20221222010822007100000002.c2x7llrlmsr3.us-east-2.rds.amazonaws.com\"\n",
    "}\n",
    "\n",
    "with psycopg.connect(**pg_connection_dict, autocommit=True) as conn:\n",
    "    with conn.cursor() as curr:\n",
    "        print(conn.info.encoding)\n",
    "        print(curr.execute(r\"SELECT now()\").fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_sql_as_dataframe(\n",
    "    conn_details: dict[str, str],\n",
    "    sql_query: Query,\n",
    "    params: Optional[Params] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Execute SQL query and return results in a DataFrame\"\"\"\n",
    "    with psycopg.connect(**pg_connection_dict, autocommit=True) as conn:\n",
    "        with conn.cursor() as curr:\n",
    "            res = curr.execute(sql_query, params).fetchall()\n",
    "            print(f\"Rows impacted: {curr.rowcount}\")\n",
    "\n",
    "            if curr.description:\n",
    "                columns = [col.name for col in curr.description]\n",
    "                return pd.DataFrame(res, columns=columns)\n",
    "            return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_sql_file(conn_details: dict[str, str], filepath: str) -> None:\n",
    "    \"\"\"Execute sql contained in a file using the conn_details\"\"\"\n",
    "    with psycopg.connect(**conn_details, autocommit=True) as conn:\n",
    "        with conn.cursor() as curr:\n",
    "            with open(filepath, \"rt\") as f:\n",
    "                contents: LiteralString = f.read()\n",
    "            if contents:\n",
    "                curr.execute(sql.SQL(contents))\n",
    "                print(f\"Rows impacted: {curr.rowcount}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_sql_statement(\n",
    "    conn_details: dict[str, str],\n",
    "    sql_statement: Query,\n",
    "    params: Optional[Params] = None,\n",
    ") -> None:\n",
    "    \"\"\"Execute sql provided using the conn_details\"\"\"\n",
    "    with psycopg.connect(**conn_details, autocommit=True) as conn:\n",
    "        with conn.cursor() as curr:\n",
    "            curr.execute(sql_statement, params)\n",
    "            print(f\"Rows impacted: {curr.rowcount}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_coordinates(conn_details: dict[str, str], coordinated_path: str):\n",
    "    \"\"\"Parse coordinates file from WSC and insert the data into the coordinates table\"\"\"\n",
    "    df = pd.read_csv(\n",
    "        coordinated_path, sep=\" \", names=[\"i\", \"j\", \"latitude\", \"longitude\"], skiprows=1\n",
    "    )\n",
    "\n",
    "    columns = [\"latitude\", \"longitude\"]\n",
    "    write_query = sql.SQL(\"COPY {table} ({columns}) FROM STDIN\").format(\n",
    "        table=sql.Identifier(\"public\", \"coordinates\"),\n",
    "        columns=sql.SQL(\", \").join(map(sql.Identifier, columns)),\n",
    "    )\n",
    "\n",
    "    with psycopg.connect(**conn_details, autocommit=True) as conn:\n",
    "        with conn.cursor() as curr:\n",
    "            with curr.copy(write_query) as copy:\n",
    "                for record in df[[\"latitude\", \"longitude\"]].values:\n",
    "                    copy.write_row(record)\n",
    "                print(f\"Rows impacted: {curr.rowcount}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_copy_statement_using_df(\n",
    "    conn_details: dict[str, str],\n",
    "    sql_statement: Query,\n",
    "    df: pd.DataFrame,\n",
    "    params: Optional[Params] = None,\n",
    ") -> None:\n",
    "    \"\"\"Execute sql copy statement against the conn_details using the DataFrame provided\"\"\"\n",
    "    with psycopg.connect(**conn_details, autocommit=True) as conn:\n",
    "        with conn.cursor() as curr:\n",
    "            with curr.copy(sql_statement, params) as copy:\n",
    "                for record in df.itertuples(index=False):\n",
    "                    copy.write_row(record)\n",
    "            print(f\"Rows impacted: {curr.rowcount}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_many_statement(\n",
    "    conn_details: dict[str, str],\n",
    "    sql_statement: Query,\n",
    "    params: Iterable[Params],\n",
    ") -> None:\n",
    "    \"\"\"Execute sql statements against the conn_details using the DataFrame provided\"\"\"\n",
    "    with psycopg.connect(**conn_details, autocommit=True) as conn:\n",
    "        with conn.cursor() as curr:\n",
    "            print(sql_statement.as_string(curr))\n",
    "            curr.executemany(sql_statement, params)\n",
    "            print(f\"Rows impacted: {curr.rowcount}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forecast_id(\n",
    "    conn_details: dict[str, str],\n",
    "    model: str,\n",
    "    forecast_reference_time: datetime,\n",
    "    forecast_step: timedelta,\n",
    "):\n",
    "    \"\"\"Select forecast_id where model, forecast_reference_time and forecast_step\"\"\"\n",
    "    fields = [\"forecast_id\"]\n",
    "    sql_statement = sql.SQL(\n",
    "        \"\"\"\n",
    "        SELECT {fields} \n",
    "        FROM {table} \n",
    "        WHERE\n",
    "            model = {model}\n",
    "            and forecast_reference_time = {time}\n",
    "            and forecast_step = {step};\"\"\"\n",
    "    ).format(\n",
    "        table=sql.Identifier(\"public\", \"forecasts\"),\n",
    "        fields=sql.SQL(\", \").join(map(sql.Identifier, fields)),\n",
    "        model=sql.Literal(model),\n",
    "        time=sql.Literal(forecast_reference_time),\n",
    "        step=sql.Literal(forecast_step),\n",
    "    )\n",
    "\n",
    "    df = execute_sql_as_dataframe(conn_details, sql_statement)\n",
    "\n",
    "    return int(df.iloc[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variable_id(\n",
    "    conn_details: dict[str, str],\n",
    "    short_name: str\n",
    "):\n",
    "    \"\"\"Select variable_id where short_name\"\"\"\n",
    "    fields = [\"variable_id\"]\n",
    "    sql_statement = sql.SQL(\n",
    "        \"\"\"\n",
    "        SELECT {fields} \n",
    "        FROM {table} \n",
    "        WHERE short_name = {name};\"\"\"\n",
    "    ).format(\n",
    "        table=sql.Identifier(\"public\", \"variables\"),\n",
    "        fields=sql.SQL(\", \").join(map(sql.Identifier, fields)),\n",
    "        name=sql.Literal(short_name),\n",
    "    )\n",
    "\n",
    "    df = execute_sql_as_dataframe(conn_details, sql_statement)\n",
    "\n",
    "    return int(df.iloc[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coordinates(\n",
    "    conn_details: dict[str, str],\n",
    "):\n",
    "    \"\"\"Select coordinates into a DataFrame\"\"\"\n",
    "    fields = [\"coord_id\", \"latitude\", \"longitude\"]\n",
    "    sql_statement = sql.SQL(\n",
    "        \"\"\"\n",
    "        SELECT {fields} \n",
    "        FROM {table};\"\"\"\n",
    "    ).format(\n",
    "        table=sql.Identifier(\"public\", \"coordinates\"),\n",
    "        fields=sql.SQL(\", \").join(map(sql.Identifier, fields)),\n",
    "    )\n",
    "\n",
    "    df = execute_sql_as_dataframe(conn_details, sql_statement)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data_to_temp_table(grib_paths:list[str], model_name:str = \"hrdps\"):\n",
    "    with psycopg.connect(**pg_connection_dict, autocommit=True) as conn:\n",
    "        with conn.cursor() as curr:\n",
    "\n",
    "            # get parameters from 1st grib file\n",
    "            grib_data = read_grib_to_df(grib_paths[0])\n",
    "            forecast = {\n",
    "                \"model\": model_name,\n",
    "                \"forecast_reference_time\": grib_data[\"forecast_reference_time\"],\n",
    "                \"forecast_step\": grib_data[\"forecast_step\"],\n",
    "            }\n",
    "            variable = {\n",
    "                \"short_name\": grib_data[\"short_name\"]\n",
    "            }\n",
    "\n",
    "            # get forecast_id\n",
    "            with conn.transaction():\n",
    "                forecast_id = curr.execute(\n",
    "                    \"\"\"\n",
    "                    SELECT forecast_id\n",
    "                    FROM public.forecasts\n",
    "                    WHERE \n",
    "                        model = %(model)s AND\n",
    "                        forecast_reference_time = %(forecast_reference_time)s AND\n",
    "                        forecast_step = %(step)s\n",
    "                \"\"\",\n",
    "                    forecast,\n",
    "                ).fetchone()\n",
    "            if not forecast_id:\n",
    "                raise ValueError(\"No forecast_id obtained\")\n",
    "\n",
    "            # get variable_id\n",
    "            with conn.transaction():\n",
    "                variable_id = curr.execute(\n",
    "                    \"\"\"\n",
    "                    SELECT variable_id\n",
    "                    FROM public.variables\n",
    "                    WHERE short_name = %s\n",
    "                \"\"\",\n",
    "                    [variable[\"short_name\"]],\n",
    "                ).fetchone()\n",
    "            if not variable_id:\n",
    "                raise ValueError(\"No variable_id obtained\")\n",
    "\n",
    "            # get coord_ids\n",
    "            with conn.transaction():\n",
    "                variable_id = curr.execute(\n",
    "                    \"\"\"\n",
    "                    SELECT variable_id\n",
    "                    FROM public.variables\n",
    "                    WHERE short_name = %s\n",
    "                \"\"\",\n",
    "                    [variable[\"short_name\"]],\n",
    "                ).fetchone()\n",
    "            if not variable_id:\n",
    "                raise ValueError(\"No variable_id obtained\")\n",
    "\n",
    "            # Prepare data to upload\n",
    "\n",
    "            curr.executemany(write_query, list(df.itertuples(index=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating fresh tables\tRows impacted: -1\n",
      "Populating coordinates table\tRows impacted: -1\n",
      "Rows impacted: 3750656\n",
      "00: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P000-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "01: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P001-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "02: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P002-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "03: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P003-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "04: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P004-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "05: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P005-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "06: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P006-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "07: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P007-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "08: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P008-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "09: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P009-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "10: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P010-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "11: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P011-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "12: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P012-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "13: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P013-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "14: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P014-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "15: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P015-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "16: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P016-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "17: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P017-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "18: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P018-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "19: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P019-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "20: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P020-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "21: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P021-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "22: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P022-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "23: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P023-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "24: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P024-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "25: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P025-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "26: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P026-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "27: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P027-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "28: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P028-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "29: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P029-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "30: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P030-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "31: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P031-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "32: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P032-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "33: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P033-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "34: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P034-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "35: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P035-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "36: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P036-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "37: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P037-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "38: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P038-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "39: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P039-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "40: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P040-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "41: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P041-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "42: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P042-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "43: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P043-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "44: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P044-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "45: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P045-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "46: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P046-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "47: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P047-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n",
      "48: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P048-00.grib2\n",
      "\tReading GRIB meta data\n",
      "\tPopulating variables table\tRows impacted: 0\n",
      "Rows impacted: 1\n",
      "\tPopulating forecasts table\tRows impacted: 1\n",
      "Rows impacted: 1\n",
      "\tPreparing predications data\n",
      "\tPopulating predictions table\tRows impacted: 3750656\n"
     ]
    }
   ],
   "source": [
    "# Create fresh tables\n",
    "print(\"Creating fresh tables\", end=\"\\t\")\n",
    "execute_sql_file(pg_connection_dict, \"../database.sql\")\n",
    "\n",
    "# Populate coordinates\n",
    "print(\"Populating coordinates table\", end=\"\\t\")\n",
    "write_coordinates(pg_connection_dict, \"../data/coordinates/hrdps_continential.txt\")\n",
    "df_coords = get_coordinates(pg_connection_dict)\n",
    "\n",
    "model = \"hrdps\"\n",
    "\n",
    "for idx, path in enumerate(paths):\n",
    "    print(f\"{idx:02}: {path}\")\n",
    "\n",
    "    # Get grib meta data from first GRIB file\n",
    "    print(\"\\tReading GRIB meta data\")\n",
    "    grib_info = read_grib_to_df(path)\n",
    "\n",
    "    # Populate variables\n",
    "    print(\"\\tPopulating variables table\", end=\"\\t\")\n",
    "    fields = [\"short_name\", \"long_name\", \"unit\"]\n",
    "    rfields = [\"variable_id\"]\n",
    "    sql_statement = sql.SQL(\n",
    "        \"\"\"\n",
    "            INSERT INTO {table}({fields}) \n",
    "            VALUES ({placeholders})\n",
    "            ON CONFLICT DO NOTHING\n",
    "        \"\"\"\n",
    "    ).format(\n",
    "        table=sql.Identifier(\"public\", \"variables\"),\n",
    "        fields=sql.SQL(\", \").join(map(sql.Identifier, fields)),\n",
    "        placeholders=sql.SQL(\", \").join(sql.Placeholder() * len(fields)),\n",
    "    )\n",
    "    execute_sql_statement(\n",
    "        pg_connection_dict,\n",
    "        sql_statement,\n",
    "        (grib_info[\"short_name\"], grib_info[\"long_name\"], grib_info[\"units\"]),\n",
    "    )\n",
    "    variable_id = get_variable_id(pg_connection_dict, grib_info[\"short_name\"])\n",
    "\n",
    "    # Populate forecasts\n",
    "    print(\"\\tPopulating forecasts table\", end=\"\\t\")\n",
    "    fields = [\"model\", \"forecast_reference_time\", \"forecast_step\"]\n",
    "    sql_statement = sql.SQL(\n",
    "        \"\"\"\n",
    "            INSERT INTO {table}({fields}) \n",
    "            VALUES ({placeholders})\n",
    "            ON CONFLICT DO NOTHING\n",
    "        \"\"\"\n",
    "    ).format(\n",
    "        table=sql.Identifier(\"public\", \"forecasts\"),\n",
    "        fields=sql.SQL(\", \").join(map(sql.Identifier, fields)),\n",
    "        placeholders=sql.SQL(\", \").join(sql.Placeholder() * len(fields)),\n",
    "    )\n",
    "    forecast_id = execute_sql_statement(\n",
    "        pg_connection_dict,\n",
    "        sql_statement,\n",
    "        (model, grib_info[\"forecast_reference_time\"], grib_info[\"forecast_step\"]),\n",
    "    )\n",
    "    forecast_id = get_forecast_id(\n",
    "        pg_connection_dict,\n",
    "        model,\n",
    "        grib_info[\"forecast_reference_time\"],\n",
    "        grib_info[\"forecast_step\"],\n",
    "    )\n",
    "\n",
    "    # Populate predictions table\n",
    "    print(\"\\tPreparing predications data\")\n",
    "    df = grib_info[\"data\"]\n",
    "    df = pd.DataFrame.merge(\n",
    "        df,\n",
    "        right=df_coords,\n",
    "        on=[\"latitude\", \"longitude\"],\n",
    "        how=\"inner\",\n",
    "        validate=\"1:1\",\n",
    "    )\n",
    "    df[\"variable_id\"] = variable_id\n",
    "    df[\"forecast_id\"] = forecast_id\n",
    "\n",
    "    fields = [\"forecast_id\", \"variable_id\", \"coord_id\", \"value\"]\n",
    "    write_query = sql.SQL(\"COPY {table} ({fields}) FROM STDIN\").format(\n",
    "        table=sql.Identifier(\"public\", \"predictions\"),\n",
    "        fields=sql.SQL(\", \").join(map(sql.Identifier, fields)),\n",
    "    )\n",
    "    print(\"\\tPopulating predictions table\", end=\"\\t\")\n",
    "    execute_copy_statement_using_df(pg_connection_dict, write_query, df.loc[:, fields])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"INSERT INTO predictions\n",
    "            (forecast_id,\n",
    "             variable_id,\n",
    "             coord_id,\n",
    "             value)\n",
    "SELECT *\n",
    "FROM   (WITH f\n",
    "             AS (SELECT forecast_id\n",
    "                 FROM   public.forecasts\n",
    "                 WHERE  model = 'hrdps'\n",
    "                        AND forecast_reference_time =\n",
    "                            '2022-12-21 07:00:00.000 -0500'\n",
    "                        AND forecast_step = '00:00:00'),\n",
    "             v\n",
    "             AS (SELECT variable_id\n",
    "                 FROM   public.variables\n",
    "                 WHERE  short_name = 'sde'),\n",
    "             c\n",
    "             AS (SELECT coord_id,\n",
    "                        latitude,\n",
    "                        longitude\n",
    "                 FROM   coordinates)\n",
    "        SELECT forecast_id,\n",
    "               variable_id,\n",
    "               coord_id,\n",
    "               t.value\n",
    "         FROM   \"temp\" t\n",
    "                cross join f\n",
    "                cross join v\n",
    "                inner join c\n",
    "                        ON t.latitude = c.latitude\n",
    "                           AND t.longitude = c.longitude)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows impacted: 49\n"
     ]
    }
   ],
   "source": [
    "sql_statement = sql.SQL(\n",
    "    \"\"\"\n",
    "        SELECT\n",
    "            c.latitude,\n",
    "            c.longitude,\n",
    "            f.forecast_reference_time + f.forecast_step AS forecast_time,\n",
    "            value AS snow_depth\n",
    "        FROM predictions p\n",
    "        inner join coordinates c on p.coord_id = c.coord_id and c.latitude = '35.758587' and c.longitude = '-127.552296'\n",
    "        inner join forecasts f on p.forecast_id = f.forecast_id\n",
    "        inner join variables v on p.variable_id = v.variable_id\n",
    "        ORDER BY forecast_time ASC\n",
    "    \"\"\")\n",
    "\n",
    "df = execute_sql_as_dataframe(pg_connection_dict, sql_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting coordinates 14:50:50 +0000\n",
      "Rows impacted: 3750656\n",
      "00: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P000-00.grib2\n",
      "\tReading GRIB meta data 14:50:57 +0000\n",
      "\tGet forecast_id 14:50:59 +0000\tRows impacted: 1\n",
      "\tGet variable_id 14:50:59 +0000\tRows impacted: 1\n",
      "\tPreparing predications data 14:50:59 +0000\n",
      "\tPopulating predictions table 14:51:18 +0000\t\n",
      "        INSERT INTO \"public\".\"test\" (\"forecast_id\", \"variable_id\", \"coord_id\", \"value 00\")\n",
      "        VALUES (%(forecast_id)s, %(variable_id)s, %(coord_id)s, %(value)s)\n",
      "        ON CONFLICT (\"forecast_id\", \"variable_id\", \"coord_id\")\n",
      "        DO UPDATE SET \"value 00\" = %(value)s;\n",
      "        \n",
      "Rows impacted: 3750656\n",
      "01: ../data/CMC_hrdps_continental_SNOD_SFC_0_ps2.5km_2022122112_P001-00.grib2\n",
      "\tReading GRIB meta data 14:54:47 +0000\n",
      "\tGet forecast_id 14:54:49 +0000\tRows impacted: 1\n",
      "\tGet variable_id 14:54:49 +0000\tRows impacted: 1\n",
      "\tPreparing predications data 14:54:49 +0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[153], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mPreparing predications data\u001b[39m\u001b[39m\"\u001b[39m, time\u001b[39m.\u001b[39mstrftime(\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m\u001b[39mH:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS +0000\u001b[39m\u001b[39m\"\u001b[39m, time\u001b[39m.\u001b[39mgmtime()))\n\u001b[1;32m     31\u001b[0m df \u001b[39m=\u001b[39m grib_info[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m---> 32\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame\u001b[39m.\u001b[39;49mmerge(\n\u001b[1;32m     33\u001b[0m     df,\n\u001b[1;32m     34\u001b[0m     right\u001b[39m=\u001b[39;49mdf_coords,\n\u001b[1;32m     35\u001b[0m     on\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mlatitude\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mlongitude\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     36\u001b[0m     how\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39minner\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     37\u001b[0m     validate\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m1:1\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     39\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mvariable_id\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m variable_id\n\u001b[1;32m     40\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mforecast_id\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m forecast_id\n",
      "File \u001b[0;32m/mnt/c/Users/mynha/Projects/shouldishovel/.venv/lib/python3.10/site-packages/pandas/core/frame.py:10090\u001b[0m, in \u001b[0;36mDataFrame.merge\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m  10071\u001b[0m \u001b[39m@Substitution\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m  10072\u001b[0m \u001b[39m@Appender\u001b[39m(_merge_doc, indents\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m  10073\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmerge\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10086\u001b[0m     validate: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m  10087\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[1;32m  10088\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mreshape\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmerge\u001b[39;00m \u001b[39mimport\u001b[39;00m merge\n\u001b[0;32m> 10090\u001b[0m     \u001b[39mreturn\u001b[39;00m merge(\n\u001b[1;32m  10091\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m  10092\u001b[0m         right,\n\u001b[1;32m  10093\u001b[0m         how\u001b[39m=\u001b[39;49mhow,\n\u001b[1;32m  10094\u001b[0m         on\u001b[39m=\u001b[39;49mon,\n\u001b[1;32m  10095\u001b[0m         left_on\u001b[39m=\u001b[39;49mleft_on,\n\u001b[1;32m  10096\u001b[0m         right_on\u001b[39m=\u001b[39;49mright_on,\n\u001b[1;32m  10097\u001b[0m         left_index\u001b[39m=\u001b[39;49mleft_index,\n\u001b[1;32m  10098\u001b[0m         right_index\u001b[39m=\u001b[39;49mright_index,\n\u001b[1;32m  10099\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m  10100\u001b[0m         suffixes\u001b[39m=\u001b[39;49msuffixes,\n\u001b[1;32m  10101\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m  10102\u001b[0m         indicator\u001b[39m=\u001b[39;49mindicator,\n\u001b[1;32m  10103\u001b[0m         validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[1;32m  10104\u001b[0m     )\n",
      "File \u001b[0;32m/mnt/c/Users/mynha/Projects/shouldishovel/.venv/lib/python3.10/site-packages/pandas/core/reshape/merge.py:110\u001b[0m, in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39m@Substitution\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mleft : DataFrame or named Series\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     94\u001b[0m \u001b[39m@Appender\u001b[39m(_merge_doc, indents\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmerge\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m     validate: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    109\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[0;32m--> 110\u001b[0m     op \u001b[39m=\u001b[39m _MergeOperation(\n\u001b[1;32m    111\u001b[0m         left,\n\u001b[1;32m    112\u001b[0m         right,\n\u001b[1;32m    113\u001b[0m         how\u001b[39m=\u001b[39;49mhow,\n\u001b[1;32m    114\u001b[0m         on\u001b[39m=\u001b[39;49mon,\n\u001b[1;32m    115\u001b[0m         left_on\u001b[39m=\u001b[39;49mleft_on,\n\u001b[1;32m    116\u001b[0m         right_on\u001b[39m=\u001b[39;49mright_on,\n\u001b[1;32m    117\u001b[0m         left_index\u001b[39m=\u001b[39;49mleft_index,\n\u001b[1;32m    118\u001b[0m         right_index\u001b[39m=\u001b[39;49mright_index,\n\u001b[1;32m    119\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m    120\u001b[0m         suffixes\u001b[39m=\u001b[39;49msuffixes,\n\u001b[1;32m    121\u001b[0m         indicator\u001b[39m=\u001b[39;49mindicator,\n\u001b[1;32m    122\u001b[0m         validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[1;32m    123\u001b[0m     )\n\u001b[1;32m    124\u001b[0m     \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result(copy\u001b[39m=\u001b[39mcopy)\n",
      "File \u001b[0;32m/mnt/c/Users/mynha/Projects/shouldishovel/.venv/lib/python3.10/site-packages/pandas/core/reshape/merge.py:713\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[39m# If argument passed to validate,\u001b[39;00m\n\u001b[1;32m    710\u001b[0m \u001b[39m# check if columns specified as unique\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[39m# are in fact unique.\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[39mif\u001b[39;00m validate \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 713\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate(validate)\n",
      "File \u001b[0;32m/mnt/c/Users/mynha/Projects/shouldishovel/.venv/lib/python3.10/site-packages/pandas/core/reshape/merge.py:1505\u001b[0m, in \u001b[0;36m_MergeOperation._validate\u001b[0;34m(self, validate)\u001b[0m\n\u001b[1;32m   1503\u001b[0m     right_unique \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39morig_right\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mis_unique\n\u001b[1;32m   1504\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1505\u001b[0m     right_unique \u001b[39m=\u001b[39m MultiIndex\u001b[39m.\u001b[39;49mfrom_arrays(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mright_join_keys)\u001b[39m.\u001b[39mis_unique\n\u001b[1;32m   1507\u001b[0m \u001b[39m# Check data integrity\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[39mif\u001b[39;00m validate \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mone_to_one\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m1:1\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m/mnt/c/Users/mynha/Projects/shouldishovel/.venv/lib/python3.10/site-packages/pandas/core/indexes/multi.py:489\u001b[0m, in \u001b[0;36mMultiIndex.from_arrays\u001b[0;34m(cls, arrays, sortorder, names)\u001b[0m\n\u001b[1;32m    486\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(arrays[i]) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(arrays[i \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]):\n\u001b[1;32m    487\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mall arrays must be same length\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 489\u001b[0m codes, levels \u001b[39m=\u001b[39m factorize_from_iterables(arrays)\n\u001b[1;32m    490\u001b[0m \u001b[39mif\u001b[39;00m names \u001b[39mis\u001b[39;00m lib\u001b[39m.\u001b[39mno_default:\n\u001b[1;32m    491\u001b[0m     names \u001b[39m=\u001b[39m [\u001b[39mgetattr\u001b[39m(arr, \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mfor\u001b[39;00m arr \u001b[39min\u001b[39;00m arrays]\n",
      "File \u001b[0;32m/mnt/c/Users/mynha/Projects/shouldishovel/.venv/lib/python3.10/site-packages/pandas/core/arrays/categorical.py:3007\u001b[0m, in \u001b[0;36mfactorize_from_iterables\u001b[0;34m(iterables)\u001b[0m\n\u001b[1;32m   3003\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(iterables) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   3004\u001b[0m     \u001b[39m# For consistency, it should return two empty lists.\u001b[39;00m\n\u001b[1;32m   3005\u001b[0m     \u001b[39mreturn\u001b[39;00m [], []\n\u001b[0;32m-> 3007\u001b[0m codes, categories \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39;49m(\u001b[39m*\u001b[39;49m(factorize_from_iterable(it) \u001b[39mfor\u001b[39;49;00m it \u001b[39min\u001b[39;49;00m iterables))\n\u001b[1;32m   3008\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(codes), \u001b[39mlist\u001b[39m(categories)\n",
      "File \u001b[0;32m/mnt/c/Users/mynha/Projects/shouldishovel/.venv/lib/python3.10/site-packages/pandas/core/arrays/categorical.py:3007\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3003\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(iterables) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   3004\u001b[0m     \u001b[39m# For consistency, it should return two empty lists.\u001b[39;00m\n\u001b[1;32m   3005\u001b[0m     \u001b[39mreturn\u001b[39;00m [], []\n\u001b[0;32m-> 3007\u001b[0m codes, categories \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m(factorize_from_iterable(it) \u001b[39mfor\u001b[39;00m it \u001b[39min\u001b[39;00m iterables))\n\u001b[1;32m   3008\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(codes), \u001b[39mlist\u001b[39m(categories)\n",
      "File \u001b[0;32m/mnt/c/Users/mynha/Projects/shouldishovel/.venv/lib/python3.10/site-packages/pandas/core/arrays/categorical.py:2980\u001b[0m, in \u001b[0;36mfactorize_from_iterable\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m   2975\u001b[0m     codes \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mcodes\n\u001b[1;32m   2976\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2977\u001b[0m     \u001b[39m# The value of ordered is irrelevant since we don't use cat as such,\u001b[39;00m\n\u001b[1;32m   2978\u001b[0m     \u001b[39m# but only the resulting categories, the order of which is independent\u001b[39;00m\n\u001b[1;32m   2979\u001b[0m     \u001b[39m# from ordered. Set ordered to False as default. See GH #15457\u001b[39;00m\n\u001b[0;32m-> 2980\u001b[0m     cat \u001b[39m=\u001b[39m Categorical(values, ordered\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m   2981\u001b[0m     categories \u001b[39m=\u001b[39m cat\u001b[39m.\u001b[39mcategories\n\u001b[1;32m   2982\u001b[0m     codes \u001b[39m=\u001b[39m cat\u001b[39m.\u001b[39mcodes\n",
      "File \u001b[0;32m/mnt/c/Users/mynha/Projects/shouldishovel/.venv/lib/python3.10/site-packages/pandas/core/arrays/categorical.py:441\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[0;34m(self, values, categories, ordered, dtype, fastpath, copy)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[39mif\u001b[39;00m dtype\u001b[39m.\u001b[39mcategories \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    440\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 441\u001b[0m         codes, categories \u001b[39m=\u001b[39m factorize(values, sort\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    442\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    443\u001b[0m         codes, categories \u001b[39m=\u001b[39m factorize(values, sort\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/mnt/c/Users/mynha/Projects/shouldishovel/.venv/lib/python3.10/site-packages/pandas/core/algorithms.py:828\u001b[0m, in \u001b[0;36mfactorize\u001b[0;34m(values, sort, na_sentinel, use_na_sentinel, size_hint)\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[39mif\u001b[39;00m na_sentinel \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    826\u001b[0m         \u001b[39m# TODO: Can remove when na_sentinel=na_sentinel as in TODO above\u001b[39;00m\n\u001b[1;32m    827\u001b[0m         na_sentinel \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m--> 828\u001b[0m     uniques, codes \u001b[39m=\u001b[39m safe_sort(\n\u001b[1;32m    829\u001b[0m         uniques, codes, na_sentinel\u001b[39m=\u001b[39;49mna_sentinel, assume_unique\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, verify\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m    830\u001b[0m     )\n\u001b[1;32m    832\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m dropna \u001b[39mand\u001b[39;00m sort:\n\u001b[1;32m    833\u001b[0m     \u001b[39m# TODO: Can remove entire block when na_sentinel=na_sentinel as in TODO above\u001b[39;00m\n\u001b[1;32m    834\u001b[0m     \u001b[39mif\u001b[39;00m na_sentinel \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/c/Users/mynha/Projects/shouldishovel/.venv/lib/python3.10/site-packages/pandas/core/algorithms.py:1866\u001b[0m, in \u001b[0;36msafe_sort\u001b[0;34m(values, codes, na_sentinel, assume_unique, verify)\u001b[0m\n\u001b[1;32m   1864\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1865\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1866\u001b[0m         sorter \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39;49margsort()\n\u001b[1;32m   1867\u001b[0m         \u001b[39mif\u001b[39;00m is_mi:\n\u001b[1;32m   1868\u001b[0m             \u001b[39m# Operate on original object instead of casted array (MultiIndex)\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m             ordered \u001b[39m=\u001b[39m original_values\u001b[39m.\u001b[39mtake(sorter)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test writing to table using arrays\n",
    "# Assume all dimension tables have already been populated\n",
    "\n",
    "# Get coordinates\n",
    "print(\"Getting coordinates\", time.strftime(\"%H:%M:%S +0000\", time.gmtime()))\n",
    "df_coords = get_coordinates(pg_connection_dict)\n",
    "model = \"hrdps\"\n",
    "\n",
    "for idx, path in enumerate(paths):\n",
    "    print(f\"{idx:02}: {path}\")\n",
    "\n",
    "    # Get grib meta data from first GRIB file\n",
    "    print(\"\\tReading GRIB meta data\", time.strftime(\"%H:%M:%S +0000\", time.gmtime()))\n",
    "    grib_info = read_grib_to_df(path)\n",
    "\n",
    "    # Get variable\n",
    "    print(\"\\tGet forecast_id\", time.strftime(\"%H:%M:%S +0000\", time.gmtime()), end=\"\\t\")\n",
    "    variable_id = get_variable_id(pg_connection_dict, grib_info[\"short_name\"])\n",
    "\n",
    "    # Get forecast\n",
    "    print(\"\\tGet variable_id\", time.strftime(\"%H:%M:%S +0000\", time.gmtime()), end=\"\\t\")\n",
    "    forecast_id = get_forecast_id(\n",
    "        pg_connection_dict,\n",
    "        model,\n",
    "        grib_info[\"forecast_reference_time\"],\n",
    "        grib_info[\"forecast_step\"],\n",
    "    )\n",
    "\n",
    "    # Populate predictions table\n",
    "    print(\"\\tPreparing predications data\", time.strftime(\"%H:%M:%S +0000\", time.gmtime()))\n",
    "    df = grib_info[\"data\"]\n",
    "    df = pd.DataFrame.merge(\n",
    "        df,\n",
    "        right=df_coords,\n",
    "        on=[\"latitude\", \"longitude\"],\n",
    "        how=\"inner\",\n",
    "        validate=\"1:1\",\n",
    "    )\n",
    "    df[\"variable_id\"] = variable_id\n",
    "    df[\"forecast_id\"] = forecast_id\n",
    "    params = [row._asdict() for row in df.loc[:, [\"forecast_id\", \"variable_id\", \"coord_id\", \"value\"]].itertuples(index=False)]\n",
    "\n",
    "    fields = [\"forecast_id\", \"variable_id\", \"coord_id\", f\"value {idx:02}\"]\n",
    "    write_query = sql.SQL(\n",
    "        \"\"\"\n",
    "        INSERT INTO {table} ({fields}, {value_field})\n",
    "        VALUES ({forecast_id}, {variable_id}, {coord_id}, {value})\n",
    "        ON CONFLICT ({fields})\n",
    "        DO UPDATE SET {value_field} = {value};\n",
    "        \"\"\").format(\n",
    "        table=sql.Identifier(\"public\", \"test\"),\n",
    "        fields=sql.SQL(\", \").join(map(sql.Identifier, fields[:-1])),\n",
    "        value_field=sql.Identifier(f\"value {idx:02}\"),\n",
    "        forecast_id=sql.Placeholder(\"forecast_id\"),\n",
    "        variable_id=sql.Placeholder(\"variable_id\"),\n",
    "        coord_id=sql.Placeholder(\"coord_id\"),\n",
    "        value=sql.Placeholder(\"value\")\n",
    "    )\n",
    "    print(\"\\tPopulating predictions table\", time.strftime(\"%H:%M:%S +0000\", time.gmtime()), end=\"\\t\")\n",
    "    execute_many_statement(pg_connection_dict, write_query, params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f40483e87a18cc8f8e88bd5a8d04335d97209f4dfc5c79189588e91c12893167"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
